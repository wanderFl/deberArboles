Primero se importa el archivo Mall_Customers (1).csv y se seleccionan las columnas “Age” y “Annual Income (k$)” como las variables explicativas (X). La idea es usar estas dos características demográficas—la edad del cliente y su ingreso anual en miles de dólares—para intentar predecir el género del cliente (“Genre”), que originalmente está registrado como texto (“Male” o “Female”). Para convertir el género en una variable numérica que el algoritmo entienda, se utiliza LabelEncoder, que asigna internamente, por ejemplo, 0 a “Female” y 1 a “Male”. De este modo, el modelo trabajará con valores numéricos en lugar de cadenas.
A continuación, se separa el conjunto completo de datos en dos partes: un 75 % para entrenamiento y un 25 % para prueba. Esto se hace mediante train_test_split de scikit-learn, fijando random_state=0 para asegurar que, si se vuelve a ejecutar el código, la partición de datos sea siempre la misma. El objetivo de esta división es que el modelo aprenda patrones a partir del 75 % de la muestra y, luego, se valide su rendimiento sobre el 25 % restante, que el modelo nunca “ha visto” durante el proceso de entrenamiento.
Antes de entrenar un Random Forest, se aplica un escalado estándar (“StandardScaler”) sobre las variables “Age” y “Annual Income (k$)”. El motivo es que Random Forest, a pesar de no requerir estrictamente que las variables estén normalizadas, se beneficia de que ambas características estén en la misma escala cuando luego exploramos visualmente los datos (sobre todo en los gráficos de frontera de decisión). El escalado convierte cada columna para que tenga media 0 y desviación estándar 1, de manera que un intervalo de edades como [18, 70] y un intervalo de ingresos como [15, 137] queden homogéneos en el espacio multidimensional.
Con los datos ya divididos y escalados, se crea un clasificador RandomForestClassifier con 10 árboles (n_estimators=10) y se elige el criterio “entropy” para medir la calidad de las particiones que cada árbol realiza. Un Random Forest es un ensamblado de varios árboles de decisión que, al entrenarse con distintas submuestras de datos y características, reduce el riesgo de sobreajuste (overfitting) que podría tener un único árbol. Cada uno de esos 10 árboles aprende a “preguntar” en sus nodos internos sobre la mejor forma de separar hombres de mujeres según los valores de edad e ingreso; a la hora de predecir, el bosque (forest) toma el voto mayoritario de los 10 árboles.
Una vez ajustado el modelo (“fit”) con X_train_scaled e y_train, se realizan dos tipos de predicciones:
Un ejemplo puntual: se le pregunta al modelo si un cliente de 30 años y con 75 k$ de ingreso anual tiende a ser “Male” o “Female”. Para ello, se escala primero ese par [30, 75] con el mismo StandardScaler que ya habíamos ajustado y luego se llama a classifier.predict. La salida numérica (0 o 1) se reconvierte en “Male”/“Female” con le.inverse_transform.
Predicciones sobre el conjunto de prueba: con classifier.predict(X_test_scaled) se obtienen las etiquetas predichas para cada uno de los datos del 25 % reservado para test. De esta forma podemos comparar directamente contra las etiquetas reales (y_test) y medir qué tan bien generaliza el modelo.
Para evaluar desempeño, se calculan dos métricas:
La matriz de confusión, que muestra cuántos hombres fueron correctamente clasificados, cuántos cuatro se etiquetaron mal, y lo mismo para mujeres. Con esa tabla 2×2 es fácil ver errores tipo I y tipo II (por ejemplo, cuántos “Male” se predijeron como “Female” y viceversa).
La precisión (accuracy), que es el porcentaje de predicciones correctas sobre el total de casos de prueba.
Luego viene la parte de visualización: se dibujan dos gráficos de contorno (uno para los datos de entrenamiento y otro para los datos de prueba) en el plano bidimensional (Age vs. Annual Income). Para ello:
Primero se regresa cada conjunto escalado a su escala original con sc.inverse_transform, de modo que los ejes “Age” e “Annual Income” tengan valores reales.
Se construye una malla (meshgrid) muy fina que cubre todo el rango de edades e ingresos presentes en los datos.
Para cada punto de esa rejilla, se predice el género utilizando el bosque entrenado (aplicando antes nuevamente el escalado).
Se colorean en rojo las zonas de la malla que el modelo asocia a “Female” y en verde las zonas que asocia a “Male”.
Finalmente, se superponen los puntos reales de clientes (rosado o verde según su etiqueta) para ver qué tan bien coinciden con las regiones coloreadas. Con esto se aprecia visualmente cómo el clasificador divide el espacio en dos regiones: en la práctica, una hipotética línea curva que separa, por decirlo así, la región rojiza de la región verdosa.
Por último, se extrae el atributo classifier.feature_importances_, que indica la importancia relativa de cada variable: en nuestro caso, cuánto aporta “Age” y cuánto aporta “Annual Income (k$)” a la toma de decisiones dentro de los árboles. Con esos valores se crea un gráfico de barras sencillo que muestra en el eje vertical el “peso” o relevancia de cada característica. Si, por ejemplo, “Annual Income” tuviera una barra muy alta y “Age” una barra muy baja, sabríamos que, para el clasificador, el ingreso anual fue mucho más determinante a la hora de distinguir hombres de mujeres que la edad.
En conjunto, este flujo cubre todo el pipeline de un problema de clasificación supervisada con Random Forest: lectura y preprocesamiento de datos, entrenamiento, predicción, evaluación cuantitativa mediante métricas, y exploración visual tanto de fronteras de decisión como de la importancia de las características. Así se asegura que el modelo no solo “funciona”, sino que también entendemos por qué toma ciertas decisiones y qué variables resultan más relevantes.